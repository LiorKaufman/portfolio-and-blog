[{"content":"\u003ch1 id=\"streamline-your-data-onboarding-with-dbt-codegen-package\"\u003eStreamline Your Data Onboarding with DBT Codegen package\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn today’s data-driven world, \u003ca href=\"https://docs.getdbt.com/\"\u003edbt\u003c/a\u003e (data build tool) has become the go-to open-source transformation tool for turning raw data into actionable insights. In this post, we’ll explore dbt’s organized layer approach (staging, intermediate, and marts/final tables) and how automation can make onboarding new sources more efficient, using a real-life example from my own experience.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDBT: Data Transformation Done Right\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDBT allows data engineers and analysts to perform complex data manipulations using a simple, maintainable, and version-controlled codebase. By treating data transformations as code, dbt enables teams to build modular, reusable, and testable data pipelines.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe Layered Approach: Staging, Intermediate, and Marts/Final Tables\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOrganizing data transformations into layers creates a clear and maintainable pipeline:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eStaging Layer:\u003c/strong\u003e The first layer ingests raw data from various sources and lightly transforms it, creating a consistent schema for further transformations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntermediate Layer:\u003c/strong\u003e Here, data from the staging layer is further transformed into a structured format, involving complex calculations, aggregations, and business logic.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMarts/Final Tables Layer:\u003c/strong\u003e The final layer consists of well-defined, curated tables optimized for end-user consumption, answering specific business questions for reporting and analytics.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eAutomating Staging with DBT-Codegen: A Real-life Example\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhen onboarding DBT in my company, I initially onboarded five tables manually. Realizing I still had 30 more tables to onboard for the specific project I was working on, I knew there had to be a more efficient way. That’s when I discovered automation using dbt-codegen (\u003ca href=\"https://github.com/dbt-labs/dbt-codegen\"\u003ehttps://github.com/dbt-labs/dbt-codegen\u003c/a\u003e) and found it significantly streamlined the process.\u003c/p\u003e\n\u003cp\u003edbt-codegen provides macros that automate the staging layer creation, such as the \u003ccode\u003egenerate_source\u003c/code\u003e macro, which generates YAML code for a source file.\u003c/p\u003e\n\u003cp\u003eHere is the official documentation from \u003ca href=\"https://github.com/dbt-labs/dbt-codegen#generate_source-source\"\u003egitlab repo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAnother useful macro provided by dbt-codegen is \u003ccode\u003egenerate_base_model\u003c/code\u003e. This macro generates the SQL code for a base model (named \u003ccode\u003estg_tablename\u003c/code\u003e) based on a source table, again here is the official documentation from the \u003ca href=\"https://github.com/dbt-labs/dbt-codegen#generate_base_model-source\"\u003egitlab repo.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUsing the Generate_Base_Model and Generate_Source Macros to Onboard Multiple Sources Efficiently\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAfter onboarding 30 sources using the \u003ccode\u003egenerate_base_model\u003c/code\u003e and \u003ccode\u003egenerate_source\u003c/code\u003e macros, I decided to onboard even more sources. For my next project, I needed to onboard about 60 sources.\u003c/p\u003e\n\u003cp\u003eAlthough copy-pasting 60times is doable, I knew there had to be a more efficient approach. By utilizing two readily available CLI tools, \u003ccode\u003esed\u003c/code\u003e and the \u003ccode\u003e\u0026gt;\u003c/code\u003e operator, I was able to save time and reduce manual work.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esed\u003c/code\u003e is a powerful stream editor for filtering and transforming text, while the \u003ccode\u003e\u0026gt;\u003c/code\u003e operator is used for output redirection, allowing you to save the output of a command to a file. By combining these two tools, I was able to run the \u003ccode\u003egenerate_source\u003c/code\u003e command as follows:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edbt run-operation generate_source \\  \n  --args \u0026#39;{\u0026#34;schema_name\u0026#34;: \u0026#34;schema_name\u0026#34;, \u0026#34;database_name\u0026#34;: \u0026#34;db_name\u0026#34;}\u0026#39; \\  \n  | sed \u0026#39;1d;2s/^.\\{14\\}//\u0026#39; \\  \n  \u0026gt; models/myfolder/sources.yml\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe sed command here uses \u003ccode\u003e1d\u003c/code\u003e to delete the first line of output, and \u003ccode\u003e2s/^.\\{14\\}//\u003c/code\u003e to remove the first 14 characters from the beginning of the second line. This effectively eliminates the first line and a specific number of characters from the second line. \u003cstrong\u003eKeep in mind that you may need to adjust the number of spaces or lines to remove based on your environment.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe output redirection operator (\u003ccode\u003e\u0026gt;\u003c/code\u003e) lets you create a file and save to it the output directly from the terminal. Using this 2 tools I did not even need to copy paste a the source file anymore.\u003cbr\u003e\nHere is the generate_base_model commands I used.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edbt run-operation generate_base_model \\  \n  --args \u0026#39;{\u0026#34;source_name\u0026#34;: \u0026#34;your_source_name\u0026#34;, \u0026#34;table_name\u0026#34;: \u0026#34;table_name\u0026#34;}\u0026#39; \\  \n  | sed \u0026#39;1,2d\u0026#39; \\  \n  \u0026gt; models/myfolder/stg_tablename.sql\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eI only had to remove the first 2 lines of the base models for each table, and while I could have manually used the same technique 60 times. I figured I could build a quick looping script to save me the manual work of changing the table name and the file name each time. Here is a template of the script I ended up writing:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003efor x in $(sed -n \u0026#39;5,$s/- name://p\u0026#39; models/myfolder/sources.yml) do   \ndbt run-operation generate_base_model --args \u0026#39;{\u0026#34;source_name\u0026#34;: \u0026#34;source_name\u0026#34;, \u0026#34;table_name\u0026#34;: \u0026#39;$x\u0026#39;}\u0026#39; \\  \n| sed 1,2d \u0026gt; models/myfolder/stg_$x.sql\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHere\u0026rsquo;s a breakdown of what each part of the script does:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003efor x in $(sed -n '5,$s/- name://p' models/myfolder/sources.yml); do \\\u003c/code\u003e: This line iterates over the output of the \u003ccode\u003esed\u003c/code\u003e command. The \u003ccode\u003esed\u003c/code\u003e command extracts table names from the \u003ccode\u003esources.yml\u003c/code\u003e file. It starts from the fifth line (\u003ccode\u003e5,\u003c/code\u003e) and goes until the end of the file (\u003ccode\u003e$\u003c/code\u003e). It looks for lines starting with \u003ccode\u003e- name:\u003c/code\u003e and removes the \u003ccode\u003e- name:\u003c/code\u003e prefix, printing only the table names. The \u003ccode\u003efor\u003c/code\u003e loop assigns each table name to the variable \u003ccode\u003ex\u003c/code\u003e and repeats the loop for each table name.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edbt run-operation generate_base_model \\\u003c/code\u003e: This line runs the \u003ccode\u003egenerate_base_model\u003c/code\u003e macro, which generates the base model SQL code for the specified source and table name.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--args '{\u0026quot;source_name\u0026quot;: \u0026quot;your_source_name\u0026quot;, \u0026quot;table_name\u0026quot;: \u0026quot;'$x'\u0026quot;}' \\\u003c/code\u003e: This line passes the source name and table name as arguments to the \u003ccode\u003egenerate_base_model\u003c/code\u003e macro. The table name is set to the variable \u003ccode\u003ex\u003c/code\u003e from the loop.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e| sed 1,2d \u0026gt; models/myfolder/stg_$x.sql; \\\u003c/code\u003e: This line pipes the output of the \u003ccode\u003egenerate_base_model\u003c/code\u003e macro to another \u003ccode\u003esed\u003c/code\u003e command, which removes the first two lines of the output (\u003ccode\u003e1,2d\u003c/code\u003e). The remaining output is then redirected to a new file named \u003ccode\u003estg_$x.sql\u003c/code\u003e, where \u003ccode\u003e$x\u003c/code\u003e is the table name. This creates a new staging model file for each table in the \u003ccode\u003esources.yml\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI hope this post was helpful for you in learning how to streamline the onboarding process of new data sources using dbt and some CLI tools.\u003c/p\u003e\n","description":"Post about using the DBT codegen package and CLI tools to automate manual DBT tasks","image":null,"permalink":"https://hugo-profile.netlify.app/blogs/dbt-source-onboarding/","title":"DBT Source automation"},{"content":"\u003ch1 id=\"checking-out-google-cloud-run-jobs-with-a-practical-example\"\u003eChecking Out Google Cloud Run Jobs With a Practical Example\u003c/h1\u003e\n\u003ch2 id=\"deploy-a-dockerized-python-script-to-google-cloud-platform-using-cloud-run-jobs\"\u003eDeploy a Dockerized Python script to Google Cloud Platform using Cloud Run Jobs.\u003c/h2\u003e\n\u003cp\u003eIn this article we will write a simple python data extraction script, deploy it to Google Cloud Run Jobs container and discuss the status of cloud run jobs — google newest container as a service offering.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrerequisites to follow this guide:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLocal installation of \u003ca href=\"https://docs.docker.com/get-docker/\"\u003eDocker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGoogle Cloud \u003ca href=\"https://cloud.google.com/sdk/docs/install\"\u003eCLI tool\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGoogle Cloud account with billing enabled\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"official-overview-of-cloud-run-jobs-from-google\"\u003eOfficial overview of Cloud Run Jobs from Google\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAlthough \u003ca href=\"https://cloud.google.com/run\"\u003eCloud Run\u003c/a\u003e services are a good fit for containers that run indefinitely listening for HTTP requests, Cloud Run jobs may be a better fit for containers that run to completion and don’t serve requests. For example, processing records from a database, processing a list of files from a Cloud Storage bucket, or a long-running operation, such as calculating Pi, would work well if implemented as a Cloud Run job.\u003c/p\u003e\n\u003cp\u003eJobs don’t have the ability to serve requests or listen on a port. This means that unlike Cloud Run services, jobs should not bundle a web server. Instead, jobs containers should exit when they are done.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"tools-we-are-going-to-use\"\u003eTools we are going to use:\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eGCloud CLI\u003c/li\u003e\n\u003cli\u003eCloud Run Jobs\u003c/li\u003e\n\u003cli\u003eCloud Container Registry\u003c/li\u003e\n\u003cli\u003eGoogle Cloud Storage (GCS)\u003c/li\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"the-script\"\u003eThe Script\u003c/h1\u003e\n\u003cp\u003eAll the code can be found at \u003ca href=\"https://github.com/LiorKaufman/cloud-run-job-demo\"\u003ehttps://github.com/LiorKaufman/cloud-run-job-demo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe are going to write a short script that will call an external API, parse the response as a pandas dataframe and save it to GCS.\u003cbr\u003e\nWill be using \u003ca href=\"https://api.publicapis.org/random\"\u003ehttps://api.publicapis.org/\u003c/a\u003e a free API resource that catalogues other free public APIs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMake a directory and files\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emkdir cloud-run-jobs-demo  \ncd cloud-run-jobs-demo   \ntouch main.py Dockerfile .dockerignore requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eSetup virtual environment\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epython3 -m venv venv  \nsource venv/bin/activate\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eInstall dependencies\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epip install requests google-cloud-storage pandas  \npip freeze \u0026gt; requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow that we have our local environment setup we can start writing some code. We need to call our API, parse the response and save it into GCS. Will be doing each step as a separate function and letting our main function run all steps.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCalling the API\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests  \nimport pandas as pddef call_api():   \n    url = \u0026#34;[https://api.publicapis.org/random](https://api.publicapis.org/random)\u0026#34;          \n    resp = requests.get(url)  \n    return resp.json()def main():  \n    print(\u0026#34;*** Starting extraction ***\u0026#34;)   \n    data = []  \n    # Each call will give us a random entry from the api   \n    for i in range(0,9):  \n        entry = call_api()  \n        data.append(entry[\u0026#34;entries\u0026#34;][0])  \n    df = pd.DataFrame(data)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eUploading the data to GCS\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBefore we can upload to GCS, we first need to create a new \u003ca href=\"https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-python\"\u003ebucket in GCP\u003c/a\u003e. In order to create a bucket we will need to have the correct IAM permissions. If you are unable to create a bucket you will have to contact you GCP administrator.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egsutil mb -p mygoogle_cloud_project_name gs://my_demo_bucket\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you are unsure if the bucket is created or want to use a different bucket you can list all available buckets using the following command.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egsutil ls -p PROJECT_ID\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow that we have a bucket we can upload our file to it. Let’s write a simple function that uploads the data we collect as a csv file.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edef upload_dataframe_as_csv(df,bucket_name, destination_blob_name):  \n    \u0026#34;\u0026#34;\u0026#34;Uploads a dataframe as csv file to the bucket.\u0026#34;\u0026#34;\u0026#34;  \n    storage_client = storage.Client()  \n    bucket = storage_client.bucket(bucket_name)  \n    blob = bucket.blob(destination_blob_name)blob.upload_from_string(df.to_csv(index=False,header=True),\u0026#34;text/csv\u0026#34;)print(  \n        f\u0026#34;{destination_blob_name} with {df.shape[0]} rows got uploaded to bucket{bucket_name}.\u0026#34;  \n    )\n\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"authentication\"\u003eAuthentication\u003c/h1\u003e\n\u003cp\u003eIn order to actually be able to upload or file into GCS we need to be authenticated to google. If you have the Gcloud CLI you can run the following command to authenticate using your own GCP user account.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egcloud auth login \n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe other auth option is using a service account key. Will create the service account and assign it’s roles using the console instead of the CLI. Navigate to the service account page under IAM \u0026amp; Admin.\u003c/p\u003e\n\u003cp\u003eClick on button to create a service account fill in the name and email. Fill up the required fields and click continue and create.\u003c/p\u003e\n\u003cp\u003eGrant the Storage Object Admin rule to the new service account and click continue and done.\u003c/p\u003e\n\u003cp\u003eTo generate the service account key:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClick the service account\u003c/li\u003e\n\u003cli\u003eGo to the Keys tab and click on the add key button\u003c/li\u003e\n\u003cli\u003eChoose key type of JSON\u003c/li\u003e\n\u003cli\u003eSave the file in a secure location on your machine\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe google storage client automatically looks for service credentials in your local development environment. You can set the a new env variable with a path to the service_account_key.json file\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eexport GOOGLE_APPLICATION_CREDENTIALS=\u0026lt;path-to-service-account-key\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOr alternatively you can run the following command to let the gcloud CLI tool deal with the auth using the service account.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egcloud auth activate-service-account --key-file \u0026lt;path-to-service-account-key\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eTesting our script\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s look at the full code we are about to test. We have our api_call function, our upload function and our main function that is coordinating all the steps.\u003cbr\u003e\nAdditionally we should have the GOOGLE_APPLICATION_CREDENTIALS variable set.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests  \nimport pandas as pd  \nfrom google.cloud import storage  \nfrom os import environTASK_INDEX = environ.get(\u0026#34;CLOUD_RUN_TASK_INDEX\u0026#34;, 0)def call_api():   \n    url = \u0026#34;[https://api.publicapis.org/random](https://api.publicapis.org/random)\u0026#34;          \n    resp = requests.get(url)  \n    return resp.json()def upload_dataframe_as_csv(df,bucket_name, destination_blob_name):  \n    \u0026#34;\u0026#34;\u0026#34;Uploads a dataframe as csv file to the bucket.\u0026#34;\u0026#34;\u0026#34;  \n    storage_client = storage.Client()  \n    bucket = storage_client.bucket(bucket_name)  \n    blob = bucket.blob(destination_blob_name)blob.upload_from_string(df.to_csv(index=False,header=True),\u0026#34;text/csv\u0026#34;)print(  \n        f\u0026#34;{destination_blob_name} with {df.shape[0]} rows got uploaded to bucket{bucket_name}.\u0026#34;  \n    )def main():  \n    print(\u0026#34;*** Starting extraction ***\u0026#34;)   \n    data = []  \n    # Each call will give us a random entry from the api   \n    for i in range(0,9):  \n        entry = call_api()  \n        data.append(entry[\u0026#34;entries\u0026#34;][0])  \n    df = pd.DataFrame(data)  \n     upload_dataframe_as_csv(df,\u0026#34;my_demo_bucket_cloud_run\u0026#34;,f\u0026#34;random_entires_{TASK_INDEX}.csv\u0026#34;)  \nif __name__ == \u0026#39;__main__\u0026#39;:  \n    main()\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWhen we run our script, we should see the following message and in the bucket itself we should see our file.\u003c/p\u003e\n\u003ch1 id=\"dockerizing-our-app-and-a-final-test\"\u003eDockerizing our app and a final test\u003c/h1\u003e\n\u003cp\u003eOur next step is to dockerize our script build a docker image and push it to google cloud container registry. Copy the following commands to your docker file.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eFROM python:3.9-slimCOPY . .# INSTALL REQUIRED LIBRARIES  \nRUN pip install -r requirements.txtCMD [\u0026#34;python\u0026#34;, \u0026#34;-u\u0026#34;, \u0026#34;main.py\u0026#34;]\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eBuild the image locally\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker build . --tag gcr.io/PROJECT-ID/JOB-NAME:latest\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo run the image successfully in our local environment we need to mount the service account key we created in the previous step. In order to successfully do that we first need to make sure we have the GOOGLE_APPLICATION_CREDENTIALS env variable is set\u003c/p\u003e\n\u003cp\u003erun the following command to set the env var and run the docker image\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eexport GOOGLE_APPLICATION_CREDENTIALS=\u0026lt;path-to-file\u0026gt;docker run --rm \\  \n -e GOOGLE_APPLICATION_CREDENTIALS=/tmp/keys/my_service_account.json \\  \n-v $GOOGLE_APPLICATION_CREDENTIALS:/tmp/keys/my_service_account.json:ro gcr.io/cloud-run-jobs-demo-354304/jobs-demo:latest\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf everything is configured correctly you should see the same success message in the terminal .\u003c/p\u003e\n\u003cp\u003eThe last step we need to do before pushing the image to the cloud is authenticating docker with gcp\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egcloud auth configure-docker gcr.io\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow we just need to push our image.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker push gcr.io/PROJECT-ID/JOB-NAME:latest\n\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"creating-and-executing-cloud-run-job\"\u003eCreating and executing cloud run job\u003c/h1\u003e\n\u003cp\u003eTo create the cloud run job we just need to run the following command\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egcloud beta run create jobs JOB-NAME --image gcr.io/PROJECT-ID/JOB-NAME:latest --region us-west4 --tasks 3\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou should be able to see a new job created in the cloud run console\u003c/p\u003e\n\u003cp\u003eTo execute a job we just run the following\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egcloud beta run jobs execute JOB-NAME\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eA very nice feature of cloud run jobs is that they can also be triggered by an API endpoint. The following snippet will deal with the authentication and make an HTTP POST call triggering the job.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eimport requests  \nfrom os import environ  \nfrom google.oauth2 import service_account  \nimport google.auth  \nimport google.auth.transport.requestsSCOPES = [  \n    \u0026#34;[https://www.googleapis.com/auth/cloud-platform](https://www.googleapis.com/auth/cloud-platform)\u0026#34;,  \n]SERVICE_ACCOUNT_FILE = environ.get(\u0026#34;GOOGLE_APPLICATION_CREDENTIALS\u0026#34;)  \ncredentials = service_account.Credentials.from_service_account_file(  \n    SERVICE_ACCOUNT_FILE, scopes=SCOPES  \n)  \nauth_req = google.auth.transport.requests.Request()  \ncredentials.refresh(auth_req)headers = {  \n\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer \u0026#34; + credentials.token,  \n\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;,  \n}  \nurl = \u0026#34;[https://us-west4-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/PROJECT-ID/jobs/JOB-NAME:run](https://us-central1-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/sightly-dev-sandbox/jobs/ad-fontes-extract-job:run)\u0026#34;  \nres = requests.post(url, headers=headers)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter triggering the job (choose whatever method works for your use case) cloud run jobs will log the output of the run and will also give a separate log per task executed.\u003c/p\u003e\n\u003cp\u003eExecuted jobsOur storage bucket after running\u003c/p\u003e\n\u003ch1 id=\"conclusions-and-notes\"\u003eConclusions and notes\u003c/h1\u003e\n\u003cp\u003eI hope this guide will prove useful if you are thinking of exploring cloud run jobs. Personally I find cloud run jobs to be an extremely convenient way to run dockerized scripts which don’t need an HTTP input. It looks to be ideal for batch processing of data, but I still want to add a few warning before deciding to use cloud run jobs.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eIt’s still in beta meaning changes can still happen.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNew tool, less documentation and tutorials (hoping to change this).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNo way to re run specific tasks (I haven’t seen any mention of this capability in the documentation )\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMy last point is more of an ask from Google.\u003cbr\u003e\nIt’s possible to wait for task completion when triggering it using the CLI.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egcloud beta run jobs execute JOB_NAME --wait\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis is great for complex workflows that require you to run multiple processes based on previous step completion. I did not find any mention of this capabilities for the job API endpoint . It’s a shame because now to use cloud run job in an orchestration tool (i.e Airflow) I would need a image with the the sdk image just to wait for the job completion vs a simple http to execute and wait.\u003c/p\u003e\n","description":"How-To dockerize and deploy a python script","image":null,"permalink":"https://hugo-profile.netlify.app/blogs/cloudrun-jobs/","title":"Cloud Run Jobs"}]